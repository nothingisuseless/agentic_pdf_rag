# üìÑ Ollama PDF-based RAG with Agentic AI

An end-to-end **Retrieval-Augmented Generation (RAG)** application built with **Python (Flask)**, **Ollama**, and **LangChain**, enabling you to upload PDFs, ingest their content, and ask contextual questions with **Agentic AI capabilities**.

## ‚ú® Features
- **PDF Upload & Ingestion** ‚Äì Extracts text from uploaded PDFs and stores embeddings.
- **Ollama LLM Integration** ‚Äì Uses local models like `llama3` for answering questions.
- **Embeddings with nomic-embed-text** ‚Äì For efficient vector search.
- **Agentic AI** ‚Äì The LLM can decide when and how to use tools to fetch answers.
- **Frontend UI** ‚Äì Simple HTML + JavaScript with model selection, temperature control, and PDF upload.
- **Lightweight** ‚Äì No external DB required (uses FAISS locally).

---

## üöÄ Getting Started

### 1Ô∏è‚É£ Prerequisites
- Python **3.10+**
- [Ollama](https://ollama.ai) installed and running locally.
- (Optional) GPU support for faster performance.

---

### Install Dependencies
```bash
git clone https://github.com/nothingisuseless/agentic_pdf_rag.git
cd YOUR_REPO_NAME

# Create a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install Python dependencies
pip install -r requirements.txt


### **Install Dependencies**
```bash
